# Part 1 Analysis  

## Overview  
The tasks in Part 1 were straightforward: take two sets of tokens from the previous assignment, generate vector embeddings for them using two separate algorithms, and analyze the results. Specifically, the goal was to identify discrepant pairs by comparing their cosine similarity scores and determine the most discrepant pair.  

## Discrepant Pairs  
Given that each pair contained over 2,000 unique tokens, it was expected that multiple discrepancies would arise. Some notable examples include:  

- **Words:** Honig and no  
  - **Skipgram Similarity:** 0.89  
  - **CBOW Similarity:** -0.43  
- **Words:** Honig and 347  
  - **Skipgram Similarity:** 0.81  
  - **CBOW Similarity:** -0.61  
- **Words:** Honig and Many  
  - **Skipgram Similarity:** 0.84  
  - **CBOW Similarity:** -0.46  
- **Words:** Honig and exist  
  - **Skipgram Similarity:** 0.85  
  - **CBOW Similarity:** -0.54  
- **Words:** Honig and 636  
  - **Skipgram Similarity:** 0.89  
  - **CBOW Similarity:** -0.56  
- **Words:** polic and ##ncy  
  - **Skipgram Similarity:** 0.98  
  - **CBOW Similarity:** -0.69  
- **Words:** polic and ##gulat  
  - **Skipgram Similarity:** 0.96  
  - **CBOW Similarity:** -0.65  
- **Words:** polic and constitution  
  - **Skipgram Similarity:** 0.92  
  - **CBOW Similarity:** -0.55  
- **Words:** polic and ##g  
  - **Skipgram Similarity:** 0.81  
  - **CBOW Similarity:** -0.49  
- **Words:** polic and Many  
  - **Skipgram Similarity:** 0.85  
  - **CBOW Similarity:** -0.65  
- **Words:** polic and action  
  - **Skipgram Similarity:** 0.87  
  - **CBOW Similarity:** -0.49  

## Factors Affecting Discrepancies  
Several factors contributed to the differences between Skip-gram and CBOW:  

1. **Training Epochs** – The more training iterations, the better the model's accuracy. After 300 epochs, a considerable amount of loss remained, though most of the reduction occurred within the first 200 epochs. The diminishing loss reduction rate suggests that further training would only provide marginal improvements.  

2. **Context and Generalization** – Some tokens, particularly numbers and word fragments, were more prone to discrepancies. For example, the numbers **347** and **636** appeared in mismatches, likely because one model interpreted them differently, and their infrequent occurrence made generalization difficult.  

3. **Semantic Understanding** – CBOW and Skip-gram process words differently, and CBOW’s tendency to smooth out context might explain the stronger negative similarities in certain cases.  

## Conclusion  
The analysis revealed notable differences in how the two models interpret tokens, especially numbers and word fragments. While training epochs and contextual limitations played a role, the discrepancies highlight the challenges of semantic generalization in embedding models.  

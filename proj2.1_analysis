# Part 1 Analysis

## Overview
The tasks in Part 1 were straightforward: take two sets of tokens from the previous assignment, generate vector embeddings for them using two separate algorithms, and analyze the results. The goal was to identify discrepant pairs by comparing their cosine similarity scores and determine the most discrepant pair.

## Discrepant Pairs
Given that each pair contained over 2,000 unique tokens, multiple discrepancies were expected. Notable examples include:

- **Words:** Honig and no  
  - **Skipgram Similarity:** 0.89  
  - **CBOW Similarity:** -0.43  

- **Words:** Honig and 347  
  - **Skipgram Similarity:** 0.81  
  - **CBOW Similarity:** -0.61  

- **Words:** Honig and Many  
  - **Skipgram Similarity:** 0.84  
  - **CBOW Similarity:** -0.46  

- **Words:** Honig and exist  
  - **Skipgram Similarity:** 0.85  
  - **CBOW Similarity:** -0.54  

- **Words:** Honig and 636  
  - **Skipgram Similarity:** 0.89  
  - **CBOW Similarity:** -0.56  

- **Words:** polic and ##ncy  
  - **Skipgram Similarity:** 0.98  
  - **CBOW Similarity:** -0.69  

- **Words:** polic and ##gulat  
  - **Skipgram Similarity:** 0.96  
  - **CBOW Similarity:** -0.65  

- **Words:** polic and constitution  
  - **Skipgram Similarity:** 0.92  
  - **CBOW Similarity:** -0.55  

- **Words:** polic and ##g  
  - **Skipgram Similarity:** 0.81  
  - **CBOW Similarity:** -0.49  

- **Words:** polic and Many  
  - **Skipgram Similarity:** 0.85  
  - **CBOW Similarity:** -0.65  

- **Words:** polic and action  
  - **Skipgram Similarity:** 0.87  
  - **CBOW Similarity:** -0.49  

## Factors Affecting Discrepancies
Several factors contributed to the differences between Skip-gram and CBOW:

1. **Training Epochs** – More training iterations typically improve model accuracy. After 300 epochs, there was still some loss, though most of the reduction occurred within the first 200 epochs, with diminishing returns thereafter.

2. **Context and Generalization** – Tokens like numbers and word fragments were more prone to discrepancies. For example, **347** and **Honig** were often mismatched, likely because their infrequent occurrences made it harder for models to generalize them.

3. **Semantic Understanding** – Skip-gram and CBOW process words differently. CBOW’s context-smoothing nature likely contributed to the stronger negative similarities in certain cases.

## Conclusion
The analysis highlighted the inherent differences in how Skip-gram and CBOW models interpret tokens, particularly numbers and word fragments. Training epochs and context availability played a significant role in these discrepancies, emphasizing the challenge of achieving consistent semantic understanding across embedding models.
